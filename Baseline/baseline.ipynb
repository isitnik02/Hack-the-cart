{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример решения с использованием статистического подхода - подсчет \n",
    "# совстречаемостей.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "hist_data = pd.read_csv('hist_data.csv')\n",
    "\n",
    "# соберем словарь встречаемостей - какие item_id покупались чаще с \n",
    "# каждым item_id \n",
    "tmp = (\n",
    "    hist_data[['item_id', 'pav_order_id']]\n",
    "    .sort_values(['item_id', 'pav_order_id'])\n",
    "    .merge(hist_data[['item_id', 'pav_order_id']], how='left', on=['pav_order_id'], suffixes=('', '_left'))\n",
    ")\n",
    "tmp = tmp[tmp['item_id'] != tmp['item_id_left']].copy()\n",
    "tmp1 = tmp.groupby(['item_id'])['item_id_left'].agg(lambda x: Counter(x).most_common(10))\n",
    "\n",
    "most_freq_dict = {k: v for (k, v) in tmp1.iteritems()}\n",
    "\n",
    "del tmp1, tmp\n",
    "gc.collect()\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# из списка кандидатов по совстречаемости удаляем повторяющиеся item_id, \n",
    "# сохраняя порядок\n",
    "def get_unique_recs(recs: list, top_n: int) -> list:\n",
    "    rec_dict = {}\n",
    "    counter = 0\n",
    "    for k, v in recs:\n",
    "        if k not in rec_dict:\n",
    "            rec_dict[k] = v\n",
    "            counter += 1\n",
    "        if counter == top_n:\n",
    "            break\n",
    "    return list(rec_dict.keys())\n",
    "\n",
    "def rec_by_item(item_id: int, most_freq_dict: dict) -> list:\n",
    "    \n",
    "    return most_freq_dict.get(item_id, None)\n",
    "\n",
    "# для каждого item_id соберем top_n самых часто встречающихся item_id, \n",
    "# отсортируем по частоте и выберем уникальные\n",
    "def rec_by_basket(basket: list, most_freq_dict: dict, top_n: int = 20) -> list:\n",
    "    \n",
    "    res = []\n",
    "    for item in basket:\n",
    "        recs = rec_by_item(item, most_freq_dict)\n",
    "        if recs is not None:\n",
    "            res += recs\n",
    "    \n",
    "    res = sorted(res, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return get_unique_recs(res, top_n)\n",
    "\n",
    "pred = test.groupby(['pav_order_id'])['item_id'].agg([('basket', list)])\n",
    "pred['preds'] = pred['basket'].map(lambda x: rec_by_basket(x, most_freq_dict=most_freq_dict))\n",
    "\n",
    "pred['preds'].to_csv('pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61650eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34481132996828945\n"
     ]
    }
   ],
   "source": [
    "# Пример использования подхода из бейзлайна для тестирования модели и \n",
    "# расчета метрики через деление hist_data на трейн и валидацию\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(data, test_size=0.3):\n",
    "    orders_sort = data[['pav_order_id', 'created']].drop_duplicates().sort_values(by=['created', 'pav_order_id'])\n",
    "    train_orders, test_orders = train_test_split(orders_sort['pav_order_id'].tolist(), test_size=test_size, shuffle=False)\n",
    "    train_orders, test_orders = set(train_orders), set(test_orders)\n",
    "    train = data[data['pav_order_id'].apply(lambda x: x in train_orders)]\n",
    "    test = data[data['pav_order_id'].apply(lambda x: x in test_orders)]\n",
    "    return train, test, orders_sort, train_orders, test_orders\n",
    "\n",
    "# из списка кандидатов по совстречаемости удаляем повторяющиеся item_id, сохраняя порядок\n",
    "def get_unique_recs(recs: list, top_n: int) -> list:\n",
    "    rec_dict = {}\n",
    "    counter = 0\n",
    "    for k, v in recs:\n",
    "        if k not in rec_dict:\n",
    "            rec_dict[k] = v\n",
    "            counter += 1\n",
    "        if counter == top_n:\n",
    "            break\n",
    "    return list(rec_dict.keys())\n",
    "\n",
    "def rec_by_item(item_id: int, most_freq_dict: dict) -> list:\n",
    "    \n",
    "    return most_freq_dict.get(item_id, None)\n",
    "\n",
    "# для каждого item_id соберем top_n самых часто встречающихся item_id, отсортируем по частоте и выберем уникальные\n",
    "def rec_by_basket(basket: list, most_freq_dict: dict, top_n: int = 20) -> list:\n",
    "    \n",
    "    res = []\n",
    "    for item in basket:\n",
    "        recs = rec_by_item(item, most_freq_dict)\n",
    "        if recs is not None:\n",
    "            res += recs\n",
    "    \n",
    "    res = sorted(res, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return get_unique_recs(res, top_n)\n",
    "\n",
    "# метрики оцениваются для вектора релевантности. пример:\n",
    "# реальные item_id, которые приобрел покупатель: [1 ,4, 5, 69]\n",
    "# рекомендованные алгоритмом item_id: [4, 6, 7, 8, 1, 2, 67, 90]\n",
    "# тогда вектор релеватности будет выглядеть следующим образом: [1, 0, 0, 0, 1, 0, 0, 0]\n",
    "# и уже по не му будет расчитываться ndcg\n",
    "def dcg(\n",
    "    y_relevance: np.ndarray\n",
    ") -> float:\n",
    "    return np.sum([(2**i - 1) / np.log2(k + 1) for (k, i) in enumerate(y_relevance, start=1)])\n",
    "\n",
    "def ndcg(\n",
    "    y_relevance: np.ndarray,\n",
    "    k: int\n",
    ") -> float:\n",
    "    if y_relevance.sum() == 0:\n",
    "        return 0.0\n",
    "    DCG = dcg(y_relevance[:k])\n",
    "    IDCG = dcg(-np.sort(-y_relevance)[:k])\n",
    "    return DCG / IDCG\n",
    "\n",
    "def apply_relevance(x):\n",
    "    return [int(item in x['basket']) for item in x['preds']]\n",
    "\n",
    "def create_relevance(pred):\n",
    "    d = pred.copy()\n",
    "    d['basket'] = d['basket'].apply(set)\n",
    "    d = d.apply(apply_relevance, axis=1)\n",
    "    return d\n",
    "\n",
    "def ndcg_full_dataset(d):\n",
    "    dd = pd.DataFrame(d.to_list()).fillna(0).to_numpy()\n",
    "    k = dd.shape[1]\n",
    "    scores = [ndcg(dd[i], k) for i in range(len(dd))]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def compute_ndcg_score(pred):\n",
    "    relevance = create_relevance(pred)\n",
    "    return ndcg_full_dataset(relevance)\n",
    "\n",
    "def make_coocurs_dict(train_data):\n",
    "    tmp = (\n",
    "        train_data[['item_id', 'pav_order_id']]\n",
    "        .sort_values(['item_id', 'pav_order_id'])\n",
    "        .merge(train_data[['item_id', 'pav_order_id']], how='left', on=['pav_order_id'], suffixes=('', '_left'))\n",
    "    )\n",
    "    tmp = tmp[tmp['item_id'] != tmp['item_id_left']].copy()\n",
    "    tmp1 = tmp.groupby(['item_id'])['item_id_left'].agg(lambda x: Counter(x).most_common(10))\n",
    "\n",
    "    most_freq_dict = {k: v for (k, v) in tmp1.iteritems()}\n",
    "\n",
    "    del tmp1, tmp\n",
    "    gc.collect()\n",
    "    return most_freq_dict\n",
    "\n",
    "def create_basket(test_data):\n",
    "    pred = test_data.groupby(['pav_order_id'])['item_id'].agg([('basket', list)])\n",
    "    return pred\n",
    "\n",
    "def make_predictions(test_data, most_freq_dict):\n",
    "    pred = create_basket(test_data)\n",
    "    pred['preds'] = pred['basket'].map(lambda x: rec_by_basket(x, most_freq_dict=most_freq_dict))\n",
    "    return pred\n",
    "\n",
    "\n",
    "# считываем исторические данные\n",
    "data = pd.read_csv(\"hist_data.csv\", parse_dates=['created'])\n",
    "\n",
    "# разобьем историю в отношении 70 на 30 для трейна и валидации\n",
    "train_data, test_data, orders_sort, train_orders, test_orders = split_data(data)\n",
    "\n",
    "# соберем словарь встречаемостей - какие item_id покупались чаще с каждым item_id \n",
    "most_freq_dict = make_coocurs_dict(train_data)\n",
    "# предсказываем\n",
    "pred = make_predictions(test_data, most_freq_dict)\n",
    "pred.to_csv(\"preds_on_splitted_hist_data.csv\")\n",
    "\n",
    "# посчитаем скор для всего набора предсказаний\n",
    "d_score = compute_ndcg_score(pred)\n",
    "print(d_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c68047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
